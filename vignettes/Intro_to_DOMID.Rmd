---
title: "Introduction to the DOMID package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to the DOMID package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## `DOMID` package - Overview
The `DOMID` (Detecting Outliers in MIxed-type Data) R package includes functions that can be used for detecting outliers in data sets consisting of mixed-type data (i.e. both continuous and discrete variables). Some of the capabilities of the package include:

- Generating artificial data sets of mixed-type data, including some marginal outliers in either the discrete or the continuous domain (or both), as well as joint outliers.
- Calculating scores of outlyingness for both the continuous and the discrete features of a data set.
- Detecting the marginal outliers in a mixed data set, when given scores of outlyingness for discrete & continuous features.
- Finding associations among discrete variables and sets of continuous features.
- Detecting joint outliers for a given association among a discrete and a set of continuous variables.

Below, we present some of the functions included in `DOMID`, so that the user can familiarise themselves with the package.

## Artificial data set generation - `gen_marg_joint_data`

Suppose that we are interested in generating an artificial data set consisting of both discrete and continuous variables. We want this data set to include some outlying observations in both the discrete and the continuous domains (i.e. marginal outliers), as well as a few joint outliers. More precisely, we generate an artificial data set with 1000 observations, 5 discrete and 5 continuous variables. We set the second discrete variable to have 4 levels and the rest to have 3 discrete levels instead. The proportion of outliers in the data set is 20%, out of which 80% are joint outliers (thus the remaining 20% of outliers will be marginal outliers). 

In order to define joint outliers, we need to impose at least one association among a discrete and a set of continuous variables. We set the first 2 discrete variables to be associated with the first 2 continuous variables each. The association type is set to `quotient` for both existing associations. As a result, projecting the data set in the space spanned by the first 2 continuous variables and colouring each observation according to the level of the first or the second discrete variable should reveal a pattern. We generate this data set using a seed number of 1 for reproducibility, store it as `dt` and we then display its structure.

```{r gen_marg_joint_data}
library(DOMID)
dt <- gen_marg_joint_data(n_obs = 1000, n_disc = 5, n_cont = 5,
                          n_lvls = c(3, 4, 3, 3, 3), p_outs = 0.20, jp_outs = 0.80,
                          assoc_target = c(1,2), assoc_vars = list(c(1, 2), c(1, 2)),
                          assoc_type = 'quotient', seed_num = 1)
str(dt)
```
As we can see, the data frame that has been generated includes 1000 observations. The first 5 variables are discrete with the number of levels we indicated plus an additional level; this level is introduced due to the generation of marginal outliers in the data set. In this case, the outliers that were generated in the discrete space will take levels that appear in very low frequencies and which are therefore indicative of an anomalous behaviour. Then, we have 5 continuous variables and finally an additional variable which is generated automatically by the function and which is a label of outlyingness. More precisely, a value of 0 indicates that the observation is not an outlier, a value of 1 indicates that the observation is a single marginal outlier (i.e. an outlier in either just the discrete or just the continuous domain), a value of 2 refers to the observation being a combined marginal outlier (i.e. an observation in both the discrete and the continuous spaces) and finally a value of 3 is assigned to joint outliers. Finally, in order to investigate whether the associations we imposed are indeed valid, we plot the first two continuous variables with points coloured according to the first and the second discrete variable levels. We make sure to discard any marginal outliers, so that the associations can be observed more easily.

```{r data_gen_plots}
plot(dt[which(dt$V11 %in% c(0,3)), c(6, 7)],
     col = dt[which(dt$V11 %in% c(0,3)), 1],
     pch = 16,
     xlab = 'Continuous Variable 1',
     ylab = 'Continuous Variable 2')
legend('topright',
       legend = c('Level 1', 'Level 2', 'Level 3'),
       col = c(1, 2, 3),
       pch = 16,
       title = 'Discrete Variable 1',
       cex = 0.9)
plot(dt[which(dt$V11 %in% c(0,3)), c(6, 7)],
     col = dt[which(dt$V11 %in% c(0,3)), 2],
     pch = 16,
     xlab = 'Continuous Variable 1',
     ylab = 'Continuous Variable 2')
legend('topright',
       legend = c('Level 1', 'Level 2', 'Level 3', 'Level 4'),
       col = c(1, 2, 3, 4),
       pch = 16,
       title = 'Discrete Variable 2',
       cex = 0.9)
```

Indeed, we see that the associations have been generated correctly. As you may observe, there exist some points which do not seem to agree with with the relationships that we imposed; these observations are the joint outliers that were generated.

## Scores of outlyingness - `disc_scores` & `cont_scores`

In order to be able to detect marginal outliers (i.e. outliers in either the discrete or the continuous space), we need to compute scores of outlyingness for the discrete and the continuous features of each observation. The function `disc_scores` takes as input a data set and the indices of the columns corresponding to the discrete variables. Notice that the discrete features must be of class `factor`; this is something that we do not need to worry about, since the function `gen_marg_joint_data` returns discrete variable columns of the aforementioned class. There is an additional parameter, `alpha`, which is the significance level of the simultaneous confidence intervals for the Multinomial proportions. The confidence intervals determine what the frequency thresholds should be for itemsets of different length, with greater `alpha` values leading to a more conservative algorithm that also penalises less infrequent itemsets. We proceed with the default value of 0.01 to construct 99% confidence intervals but any positive real value at most equal to 0.20 can be given. The final parameter is `MAXLEN`, which is the itemset sequence length to be considered for discrete scores. The default value is 0 which calculates MAXLEN according to a criterion on the sparsity caused by the total combinations that can be encountered as sequences of greater length are taken into account. Otherwise, MAXLEN can take any value from 1 up to the total number of discrete variables included in the data set. We proceed with the default value here again. The output of `disc_scores` is a list of 3 elements. The first element is the parameter `MAXLEN` that is used for computing discrete scores. The second element in the list is a data frame with the discrete scores of outlyingness. Finally, the last element in the output list is a matrix of dimensions `nrow(data) x length(disc_cols)`, including the contribution of each discrete feature to the discrete score of outlyingness of each observation.

```{r disc_scores}
discrete_scores <- disc_scores(data = dt, disc_cols = c(1:5), alpha = 0.01, MAXLEN = 0)
# MAXLEN
discrete_scores[[1]]
# Discrete scores for observations 61-70
discrete_scores[[2]][c(61:70), ]
# Contributions of discrete features for observations 61-70
discrete_scores[[3]][c(61:70), ]
```

Computing the continuous scores of outlyingness is easily done using the `cont_scores` function. The function makes use of the Extended Isolation Forest algorithm from the `isotree` package. The default hyperparameter values for the Extended Isolation Forest algorithm are `sample_size = 256`, `ntrees = 500`, `max_depth = 100` and finally `ndim = 0`, corresponding to `ndim = length(cont_cols)`. The output is a data frame with the continuous score for each observation, just like the 2nd element of the list that the `disc_scores` function returns.

```{r cont_scores}
continuous_scores <- cont_scores(data = dt, cont_cols = c(6:10), sample_size = 256,
                                 ntrees = 500, ndim = 0, max_depth = 100, seed_num = 1)
# Continuous scores for first 10 observations
continuous_scores[c(1:10), ]
```

We can finally generate a "score profile" plot to see if the scores computed agree with whether each observation is an outlier or not. Indeed, we can see that marginal outliers appear to have higher discrete and continuous scores than inliers (i.e. non-outliers) and joint outliers, which are by definition not expected to have large scores of outlyingness.

```{r score_profile}
# Add everything in a data frame
score_profile_dt <- data.frame('Type' = as.factor(dt[, 11]),
                               'Discrete_Score' = discrete_scores[[2]][, 2],
                               'Continuous_Score' = continuous_scores[, 2])
# Score profile plot
score_profile_dt <- score_profile_dt[order(score_profile_dt$Type), ]
par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)
plot(score_profile_dt[, c(2 ,3)],
     col = score_profile_dt[, 1],
     pch = 16,
     xlab = 'Discrete Score',
     ylab = 'Continuous Score',
     main = "Score profile plot for generated data set")
legend("topright",
       inset=c(-0.325,0),
       legend=c("Inlier",
                "Single Marginal",
                "Combined Marginal",
                "Joint"),
       col = c(1, 2, 3, 4),
       pch=16,
       title="Outlier Type",
       cex = 0.85)
```

## Detecting marginal outliers - `marg_outs_scores` & `marg_outs`

In order to detect the marginal outliers in a data frame, we need to make use of the discrete and the continuous scores of outlyingness. The function `marg_outs_scores` uses the scores (both discrete & continuous), as well as the matrix of contributions and detects the observations with scores higher than the majority of data points, which are therefore much more likely outlying in either space. There are 3 additional parameters; `alpha`, `rho` and `epsilon`; the first one was already explained in `disc_scores` while the last 2 represent the maximum proportion of outliers that we believe exist in the data set and the additional proportion of outliers that we are willing to tolerate, respectively. It holds that $\rho \in [0, 0.5]$, $\epsilon \in [0, 0.25)$ with $\rho > \epsilon$ and $\rho + \epsilon \leq 0.5$, as $\epsilon$ is just an additional "error" that we allow the algorithm to make. If the true proportion of outliers is equal to $\rho + \epsilon$, then this restricts the algorithm a lot as it does not give it the flexibility to allow for a few inliers to be regarded as outliers, thus we recommend setting this sum to be equal to something reasonable. For the simulations below we set it to 0.22 by specifying `rho = 0.20` and `epsilon = 0.02` (also the default values). We also set the `alpha` value to its default of 0.01 to ensure the algorithm is not very conservative with the discrete scores. The output is a list consisting of 3 vectors, the first 2 corresponding to the row indices of outlying observations in just the discrete or just the continuous space and the 3rd corresponding to the combined marginal outliers (i.e. observations outlying in both domains). We use this function on our data frame to see if the observations flagged as marginal outliers are indeed so.

```{r marg_outs_scores}
marginal_outliers <- marg_outs_scores(data = dt,
                                      disc_cols = c(1:5),
                                      outscorediscdf = discrete_scores[[2]],
                                      outscorecontdf = continuous_scores,
                                      outscorediscdfcells = discrete_scores[[3]],
                                      alpha = 0.01,
                                      rho = 0.20, epsilon = 0.02)
table(dt[unique(unlist(marginal_outliers)), 11])
```
Indeed, our data set included 23 single marginal and 16 combined marginal outliers, meaning that the function has managed to detect these, while not erroneously flagging any inliers or joint outliers. An alternative option is to use the `marg_outs` function instead. This function only takes as input variables the data frame, the indices of the discrete variable columns and the indices of the continuous variable columns. It returns the same output as `marg_outs_scores`. In fact, `marg_outs` uses `marg_outs_scores`; their difference is that the former does not require inputting discrete or continuous scores of outlyingness, as these are calculated automatically using `disc_scores` and `cont_scores`. Using `marg_outs_scores` offers additional flexibility; for example, the user can set the hyperparameter values for the Extended Isolation Forest algorithm used in `cont_scores`, whereas `marg_outs` will use the aforementioned function with its default hyperparameter values (except for `alpha`, `MAXLEN`, `rho` and `epsilon` which can be specified by the user). In this case, all hyperparameters involved in the calculation of the discrete and the continuous scores were set equal to their default values, so we expect `marg_outs` to return identical results to `marg_outs_scores`, as we show below.

```{r marg_outs, eval = FALSE}
marginal_outliers_2 <- marg_outs(data = dt,
                                 disc_cols = c(1:5),
                                 cont_cols = c(6:10),
                                 alpha = 0.01, MAXLEN = 0,
                                 rho = 0.20, epsilon = 0.02)
all(unique(unlist(marginal_outliers)) == unique(unlist(marginal_outliers_2)))
```

## Detecting associations between variables - `assoc_detect`

Once we have detected the marginal outliers in our data set, we can look for joint outliers as well. These are defined as "innocent-looking" observations with their discrete features taking typical levels, while their continuous values are not too far away from the rest of the data points. As a result, their discrete and continuous scores are low enough and they cannot be detected together with the marginal outliers. However, these observations are outlying in the sense that they do not follow an existing "pattern" or "association" between features of distinct types. In practice, these associations will typically be defined between a discrete and a set of continuous features and they will (in many cases) be unknown to the user. Thus, we seek to detect these associations somehow.

The function `assoc_detect` aims at detecting associations between a discrete and a set of continuous variables, using the Kruskal Wallis H test and then a nearest neighbours approach for the most centrally located point of each level. The number of nearest neighbours considered for each level is given by the product of the parameter `delta` (which we set equal to 0.10) and the number of observations possessing that level upon the removal of marginal outliers. The distance metric we use a weighted L1 norm, therefore the order of the Minkowski distance is set equal to a unit. For the Kruskal-Wallis H test we set a significance level `alpha1` equal to $10^{-3}$ and then for the goodness of fit tests used for the distribution of the nearest neighbours, we set a significance level `alpha2` value of $10^{-1}$. We will look for associations for all 5 discrete variables.

```{r assoc_detect}
assoctns1 <- assoc_detect(data = dt,
                          marginals = unique(unlist(marginal_outliers)),
                          target_inx = 1,
                          pred_inx = c(6:10),
                          delta = 0.50,
                          mink_order = 1,
                          alpha1 = 1e-3,
                          alpha2 = 1e-2)
assoctns2 <- assoc_detect(data = dt,
                          marginals = unique(unlist(marginal_outliers)),
                          target_inx = 2,
                          pred_inx = c(6:10),
                          delta = 0.50,
                          mink_order = 1,
                          alpha1 = 1e-3,
                          alpha2 = 1e-2)
assoctns3 <- assoc_detect(data = dt,
                          marginals = unique(unlist(marginal_outliers)),
                          target_inx = 3,
                          pred_inx = c(6:10),
                          delta = 0.50,
                          mink_order = 1,
                          alpha1 = 1e-3,
                          alpha2 = 1e-2)
assoctns4 <- assoc_detect(data = dt,
                          marginals = unique(unlist(marginal_outliers)),
                          target_inx = 4,
                          pred_inx = c(6:10),
                          delta = 0.50,
                          mink_order = 1,
                          alpha1 = 1e-3,
                          alpha2 = 1e-2)
assoctns5 <- assoc_detect(data = dt,
                          marginals = unique(unlist(marginal_outliers)),
                          target_inx = 5,
                          pred_inx = c(6:10),
                          delta = 0.50,
                          mink_order = 1,
                          alpha1 = 1e-3,
                          alpha2 = 1e-2)
```

As we can see, the function has successfully detected that we have an association between the first and the second discrete variables and the first 2 continuous features, while the remaining three discrete variables are not associated with the continuous attributes. Therefore, we can now proceed with the detection of joint outliers which can only exist in the space spanned by the first 2 continuous and the first or the second discrete variables.

## Detecting joint outliers - `kde_classif`, `elbow_angle`, `consec_angles` & `joint_outs`

Based on our findings above, it is safe to assert that there exists an association between the first and the second discrete variables and the first 2 continuous features. However, we still need to devise a method for detecting the observations (if any) which do not conform to the pattern that has been identified. Our strategy involves using KDE classification and looking at the misclassified observations among the data points which were not flagged as marginal outliers. In our case, the classes are overlapping - indeed, if we look back at the projection of the data in the first 2 continuous features that we plotted once we generated our data, the classes are separable but points on the boundaries can also be seen to "blend" with the neighbouring class. As a result of this "overlap", we expect a much larger number of misclassifications than the actual number of joint outliers. At the same time, these points close to the boundary are misclassified but the values of the kernel density estimators under the true and the falsely predicted classes will not differ much. Thus, we define the KDE ratio:

$$\Lambda_i = \frac{\max\limits_{l=1, \dots, \ell_j}\hat{f}_l\left(\boldsymbol{X}_{D_j} \mid \boldsymbol{X}_{i, C_\mathcal{J}}\right)}{\hat{f}_{l^\mathrm{true}}\left(\boldsymbol{X}_{D_j} \mid \boldsymbol{X}_{i, C_\mathcal{J}}\right)}.$$

The KDE ratio will be equal to a unit if the maximum kernel density estimator is achieved for the true level $l^\mathrm{true}$, otherwise it will exceed 1. Notice that $\hat{f}_l$ refers to the kernel density estimator for the $l$th level of a discrete variable (in this case $\boldsymbol{X}_{D_j}$), while $\boldsymbol{X}_{i, C_\mathcal{J}}$ is the vector of values of the continuous variables $C_\mathcal{J}$ which are associated with the $j$th discrete variable, for the $i$th observation.

Our goal is to find an optimal threshold value $\Lambda^*_i$ value, such that the joint outliers are the misclassified observations for which $\Lambda_i > \Lambda^*_i$. This way, we can restrict the number of misclassified points close to the borders to be treated as joint outliers. The function `kde_classif` performs KDE classification of a discrete variable given a set of associated continuous features. The `locfit` package is used when the set of predictors includes more than 1 variable, otherwise the `density` function is used (from the `stats` package) for univariate densities. The function requires specifying which observations are marginally outlying, so that these are discarded from the sample. Moreover, it lets the user specify the kernel to be used (by default the Gaussian kernel is used), as well as the value of the parameter `alpha_val` which is related to the adaptive nearest neighbours bandwidth that is used by `locfit` (see the package documentation for more details). Notice that these arguments are ignored if the set of predictors consists of 1 variable as a Gaussian KDE is used instead, with Silverman's rule of thumb used to determine the bandwidth (see documentation of `density` for more information). Finally, `kde_classif` requires a threshold parameter `Lambda_i`; this can either be a number at least equal to 1, the value 0 (default), or a vector of values. Setting this equal to a value of 1 will simply return the misclassified observations, then any value larger than that will impose the additional constraint that $\Lambda_i$ should be greater than the value of `Lambda_i` and will return these observations instead. The default value `Lambda_i = 0` will return the misclassified observations for $\Lambda^*_i = 1, 1.5, \dots, 20$. Any other vector will return the misclassified points for various threshold values specified. We set `Lambda_i = 0` and then we plot the number of misclassified observations for each $\Lambda^*_i$.

```{r kde_classif}
kde_classifications <- kde_classif(data = dt,
                                   target_inx = 1,
                                   pred_inx = c(6, 7),
                                   marg_outs = unique(unlist(marginal_outliers)),
                                   Lambda_i = 0,
                                   kernel = "gauss",
                                   alpha_val = 0.3)
plot(x = seq(1, 20, by =.5), y = kde_classifications[[1]],
     type = 'l', lwd = 2, col = 'navy',
     xlab = expression(Lambda[i]~"*"),
     ylab = "Misclassified Observations",
     main = "Misclassified observations for varying threshold values")
```

The plot above reveals that a suitable $\Lambda^*_i$ value should be somewhere around 3. We have introduced the Method of Consecutive Angles, which is capable of determining a $\Lambda^*_i$ value that gives reasonable results using the angles between consecutive line segments that join the amount of misclassified points for consecutive $\Lambda^*_i$ values. This method, implemented in the function `consec_angles`, requires a vector of observed values, the range of values for which these observations were made (in this case that is the sequence of values from 1 up to 20 in steps of half a unit) and 2 tolerance parameters. The first one, `drop_tol`, accounts for the fact that the rate of decrease of the misclassified observations for 2 consecutive $\Lambda^*_i$ values may be the same but if the actual different (or drop) exceeds a tolerated level `drop_tol`, it cannot be considered as insignificant. We use the default value of `drop_tol = 3`. Then, `range_tol` controls the maximum value of $\Lambda^*_i$ for which we want to use the method of consecutive angles. Here, we take the default `range_tol = 21` which corresponds to the 21st element of the sequence 1, 1.5, $\dots$, 20. This is the value $\Lambda^*_i = 11$, meaning that if the method of consecutive angles finds a $\Lambda^*_i$ value over 11, it will return the elbow point of the curve instead, using the `kneedle` algorithm. We calculate $\Lambda_i^*$ for our data set below. We also output the indices for the observations with a KDE ratio over the $\Lambda^*_i$ value returned.

```{r consec_angles}
Lambda_star <- consec_angles(vec = kde_classifications[[1]],
                             range = seq(1, 20, by = .5),
                             drop_tol = 3,
                             range_tol = 21)
print(Lambda_star)

# Use Lambda_star
Lambda_star_inx <- match(Lambda_star, seq(1, 20, by = .5))
# Joint outliers detected
joint_outs_det <- kde_classifications[[2]][[Lambda_star_inx]]
print(length(joint_outs_det))
summary(dt[joint_outs_det, ])
```
Indeed we can see above that just a few observations remained by demanding that the KDE ratio exceeds the $\Lambda^*_i$ value found by the method of consecutive angles and all these are joint outliers. However, the method of consecutive angles is not always ideal; in fact, it turns out that in some instances it may be better to "sacrifice" some inliers just for the sake of not missing many joint outliers. In any case, we provide the user with the `elbow_angle` function, which returns the value of the angle (in degrees) between the line segments joining the number of misclassified observations for $\Lambda_i^* = 1$ and $\Lambda_i^* = \Lambda_\mathrm{elbow}$ and for $\Lambda_i^* = \Lambda_\mathrm{elbow}$ and $\Lambda_i^* = 20$. Notice that $\Lambda_\mathrm{elbow}$ is the value of $\Lambda_i$ for which an elbow in the curve of misclassified points is being observed. This value turns out to be very useful for determining the method to be used, with the number of discrete levels of the target discrete feature being of crucial importance. If the number of levels is larger than 5 for instance, the method of consecutive angles is the best option but for a smaller number of levels, going for $\Lambda_i^*$ equal to a small value above 1 returns better results. As an illustration, we calculate the elbow angle for our misclassifications.

```{r elbow_angle}
angle <- elbow_angle(vec = kde_classifications[[1]],
                     range = seq(1, 20, by = .5))
print(angle)
```

Now, we are ready to detect the joint outliers in our data set. The function `joint_outs` can detect the joint outliers for given associations between discrete and continuous features. The parameter `assoc_target` takes as input a vector of target discrete variables, which have been found to be associated with sets of continuous variables. Then, `assoc_vars` will be a list including the corresponding vectors of the continuous features associated with each target discrete variable that was earlier defined in `assoc_target`. The `method` argument can take 3 levels; `"consec_angles"`, `"conservative"` or `"bin"`. The first option will choose a suitable value for the threshold parameter $\Lambda_i^*$ using the method of consecutive angles, as described earlier. The option `"conservative"` sets by default $\Lambda^*_i = 3$, which is a rather conservative choice that is found to perform well in some cases and finally `method = "bin"` refers to the case of a binary target variable, where $\Lambda^*_i = 2$ is chosen instead, due to poor performance that has been empirically observed for higher threshold values. The parameters `drop_tol` and `range_tol` can also be specified by the user. The `joint_outs` function will essentially make use of `kde_classif` with the default hyperparameter choices for `kernel` and `alpha_val`. If `method` is set equal to `"consec_angles"`, the parameter `Lambda_i` will also be set to its default of 0, otherwise for `method = "conservative"` or for `method = "bin"`, `Lambda_i` will be set equal to 3 or 2, respectively. Them `drop_tol` and `range_tol` can be specified by the user. The output of `kde_classif` will be returned if `method = "conservative"` or if `method = "bin"`, while for `method = "consec_angles"`, the `consec_angles` function will be used to determine what a suitable $\Lambda^*_i$ value should be and the output corresponding to this value will be given to the user. However, if the user wishes to select their preferred `method` for each association according to the value of the elbow angle, this process needs to be done manually and the `elbow_angle` function should be used as well. In order to wrap things up, we finally detect the joint outliers based on the associations we found between the first and the second discrete features with the first 2 continuous variables. The results reveal that all joint outliers were detected, alongside a very small number of inliers which were falsely flagged to be joint outliers.

```{r joint_outs}
joint_outliers <- joint_outs(data = dt,
                             marg_outs = unique(unlist(marginal_outliers)),
                             assoc_target = c(1, 2),
                             assoc_vars = list(c(6, 7), c(6, 7)),
                             method = "consec_angles",
                             drop_tol = 3,
                             range_tol = 21)
print(length(joint_outliers))
table(dt[joint_outliers, 11])
```

## A wrapper function for detecting outliers in mixed-type data - `DOMID`

The `DOMID` function serves as a wrapper function for all the functions presented above. It returns a list that includes the row indices for marginal outliers as its first 3 elements (making the distinction between discrete, continuous and combined marginal outliers) and a vector of row indices for the joint outliers (assuming an association has been found). It further returns the discrete and continuous scores for the observations, the matrix of contributions and finally the value of `MAXLEN` used for the calculation of discrete scores. We test this on our data frame but we recommend the use of the functions above for a more detailed and thorough analysis. We also check whether the output is the same as the one we got earlier.

```{r DOMID_test, eval=FALSE}
outliers_detected <- DOMID(data = dt, disc_cols = c(1:5), cont_cols = c(6:10),
                           alpha = 0.01, MAXLEN = 0, rho = 0.2, epsilon = 0.02,
                           sample_size = 256, ntrees = 500, ndim = 0,
                           max_depth = 100, seed_num = 1, delta = 0.5,
                           mink_order = 1, alpha1 = 1e-3, alpha2 = 1e-2,
                           method = "consec_angles", drop_tol = 3,
                           range_tol = 21)
all(c(unique(unlist(marginal_outliers)), joint_outliers) %in% unique(unlist(outliers_detected[1:4])))
all(unique(unlist(outliers_detected[1:4])) %in% c(unique(unlist(marginal_outliers)),
                                                  joint_outliers))
```

## Summary

We have introduced the `DOMID` (Detecting Outliers in MIxed-type Data) R package and have shown how it can be used for finding outlying observations in a data set consisting of mixed-type data (i.e. continuous and discrete features). We have used an artificially generated data set as an example on which the main functions were applied. The main functions included in the package are described here but we encourage the user to read the documentation should anything be unclear.
